import argparse
from functools import partial
from itertools import repeat
import multiprocessing
import logging
import os

import json_tricks
from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder
from ray.rllib.offline.json_writer import JsonWriter
from tqdm import tqdm

from gym_recorder.compressor import decompress_data


def process_file(file_path, input_folder, output_folder):
    if not file_path.endswith(".jsonl"):
        return

    with open(os.path.join(input_folder, file_path), "r") as f:
        batch_builder = SampleBatchBuilder()
        writer = JsonWriter(
            output_folder,
            compress_columns=[
                "t",
                "eps_id",
                "agent_index",
                "obs",
                "actions",
                "action_prob",
                "action_logp",
                "rewards",
                "dones",
                "new_obs",
            ],
        )

        # each line on this file is an episode
        episode_s = f.readline()
        while episode_s:
            episode = json_tricks.loads(episode_s, ignore_comments=False)
            if type(episode) is str:
                episode = decompress_data(episode)

            for t, obs, action, new_obs, reward, done in zip(
                range(len(episode["obs"])),
                episode["obs"],
                episode["action"],
                episode["new_obs"],
                episode["reward"],
                episode["done"],
            ):
                # any MDP-related reprocessing would happen here

                # add to batch
                batch_builder.add_values(
                    t=t,
                    eps_id=episode["episode_id"],
                    agent_index=episode["agent_id"],
                    obs=obs,
                    actions=action,
                    action_prob=1.0,  # put the true action probability here
                    action_logp=0.0,
                    rewards=reward,
                    dones=done,
                    new_obs=new_obs,
                )

            # write file & load new episode
            writer.write(batch_builder.build_and_reset())
            episode_s = f.readline()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num-workers", type=int, default=multiprocessing.cpu_count())
    parser.add_argument(
        "-i",
        "--input-folder",
        type=str,
        required=True,
        help="Input folder containing .jsonl files generated by the wrapper.",
    )
    parser.add_argument(
        "-o",
        "--output-folder",
        type=str,
        required=True,
        help="Output folder for Ray Datasets-compatible data.",
    )
    parser.add_argument(
        "-v",
        "--verbosity",
        type=int,
        default=logging.INFO,
        help="Verbosity level (an integer compatible with logging module).",
    )
    args = parser.parse_args()

    logging.basicConfig(level=args.verbosity)

    input_files = os.listdir(args.input_folder)
    logging.info(f"Found {len(input_files)} files to process in `{args.input_folder}`.")

    pool = multiprocessing.Pool(args.num_workers)
    pool.map(
        partial(
            process_file,
            input_folder=args.input_folder,
            output_folder=args.output_folder,
        ),
        input_files,
    )
    pool.close()
    pool.join()
    logging.info("Finished processing files")
